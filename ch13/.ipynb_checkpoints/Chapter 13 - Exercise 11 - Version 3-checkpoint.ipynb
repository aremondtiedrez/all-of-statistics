{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a59cab17",
   "metadata": {},
   "source": [
    "Get the Coronary Risk-Factor Study (CORIS)\n",
    "data from the book web site (https://www.stat.cmu.edu/~larry/all-of-statistics/=data/coris.dat).\n",
    "Use backward stepwise logistic regression based on AIC to select a model.\n",
    "Summarize your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5e894c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tabulate import tabulate\n",
    "\n",
    "import functools # For the reduce() function\n",
    "import more_itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.special # For the expit() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d944cbe5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row.names</th>\n",
       "      <th>sbp</th>\n",
       "      <th>tobacco</th>\n",
       "      <th>ldl</th>\n",
       "      <th>adiposity</th>\n",
       "      <th>famhist</th>\n",
       "      <th>typea</th>\n",
       "      <th>obesity</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>age</th>\n",
       "      <th>chd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>160</td>\n",
       "      <td>12.00</td>\n",
       "      <td>5.73</td>\n",
       "      <td>23.11</td>\n",
       "      <td>1</td>\n",
       "      <td>49</td>\n",
       "      <td>25.30</td>\n",
       "      <td>97.20</td>\n",
       "      <td>52</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>144</td>\n",
       "      <td>0.01</td>\n",
       "      <td>4.41</td>\n",
       "      <td>28.61</td>\n",
       "      <td>0</td>\n",
       "      <td>55</td>\n",
       "      <td>28.87</td>\n",
       "      <td>2.06</td>\n",
       "      <td>63</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>118</td>\n",
       "      <td>0.08</td>\n",
       "      <td>3.48</td>\n",
       "      <td>32.28</td>\n",
       "      <td>1</td>\n",
       "      <td>52</td>\n",
       "      <td>29.14</td>\n",
       "      <td>3.81</td>\n",
       "      <td>46</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>170</td>\n",
       "      <td>7.50</td>\n",
       "      <td>6.41</td>\n",
       "      <td>38.03</td>\n",
       "      <td>1</td>\n",
       "      <td>51</td>\n",
       "      <td>31.99</td>\n",
       "      <td>24.26</td>\n",
       "      <td>58</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>134</td>\n",
       "      <td>13.60</td>\n",
       "      <td>3.50</td>\n",
       "      <td>27.78</td>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "      <td>25.99</td>\n",
       "      <td>57.34</td>\n",
       "      <td>49</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>457</th>\n",
       "      <td>458</td>\n",
       "      <td>214</td>\n",
       "      <td>0.40</td>\n",
       "      <td>5.98</td>\n",
       "      <td>31.72</td>\n",
       "      <td>0</td>\n",
       "      <td>64</td>\n",
       "      <td>28.45</td>\n",
       "      <td>0.00</td>\n",
       "      <td>58</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>458</th>\n",
       "      <td>459</td>\n",
       "      <td>182</td>\n",
       "      <td>4.20</td>\n",
       "      <td>4.41</td>\n",
       "      <td>32.10</td>\n",
       "      <td>0</td>\n",
       "      <td>52</td>\n",
       "      <td>28.61</td>\n",
       "      <td>18.72</td>\n",
       "      <td>52</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>459</th>\n",
       "      <td>460</td>\n",
       "      <td>108</td>\n",
       "      <td>3.00</td>\n",
       "      <td>1.59</td>\n",
       "      <td>15.23</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>20.09</td>\n",
       "      <td>26.64</td>\n",
       "      <td>55</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>460</th>\n",
       "      <td>461</td>\n",
       "      <td>118</td>\n",
       "      <td>5.40</td>\n",
       "      <td>11.61</td>\n",
       "      <td>30.79</td>\n",
       "      <td>0</td>\n",
       "      <td>64</td>\n",
       "      <td>27.35</td>\n",
       "      <td>23.97</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>461</th>\n",
       "      <td>462</td>\n",
       "      <td>132</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4.82</td>\n",
       "      <td>33.41</td>\n",
       "      <td>1</td>\n",
       "      <td>62</td>\n",
       "      <td>14.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>46</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>462 rows Ã— 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     row.names  sbp  tobacco    ldl  adiposity  famhist  typea  obesity  \\\n",
       "0            1  160    12.00   5.73      23.11        1     49    25.30   \n",
       "1            2  144     0.01   4.41      28.61        0     55    28.87   \n",
       "2            3  118     0.08   3.48      32.28        1     52    29.14   \n",
       "3            4  170     7.50   6.41      38.03        1     51    31.99   \n",
       "4            5  134    13.60   3.50      27.78        1     60    25.99   \n",
       "..         ...  ...      ...    ...        ...      ...    ...      ...   \n",
       "457        458  214     0.40   5.98      31.72        0     64    28.45   \n",
       "458        459  182     4.20   4.41      32.10        0     52    28.61   \n",
       "459        460  108     3.00   1.59      15.23        0     40    20.09   \n",
       "460        461  118     5.40  11.61      30.79        0     64    27.35   \n",
       "461        462  132     0.00   4.82      33.41        1     62    14.70   \n",
       "\n",
       "     alcohol  age  chd  \n",
       "0      97.20   52    1  \n",
       "1       2.06   63    1  \n",
       "2       3.81   46    0  \n",
       "3      24.26   58    1  \n",
       "4      57.34   49    1  \n",
       "..       ...  ...  ...  \n",
       "457     0.00   58    0  \n",
       "458    18.72   52    1  \n",
       "459    26.64   55    0  \n",
       "460    23.97   40    0  \n",
       "461     0.00   46    1  \n",
       "\n",
       "[462 rows x 11 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the data into a pandas data frame\n",
    "coris_df = pd.read_csv('../data/coris_clean.dat')\n",
    "\n",
    "# Print the data frame, as a sanity check\n",
    "coris_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "188e458f",
   "metadata": {},
   "source": [
    "### Performing the weighted least squares minimization\n",
    "\n",
    "Note that performing the weighted least squares minimization\n",
    "$$\n",
    "    \\beta = \\text{argmin} {|| \\mathbb{Z} - \\mathbb{X}\\beta ||}_{ \\mathbb{W} }^2\n",
    "$$\n",
    "is equivalent to performing the *unweighted* least squares minimization\n",
    "$$\n",
    "    \\beta = \\text{argmin} {|| \\mathbb{W}^{1/2} \\mathbb{Z} - \\mathbb{W}^{1/2} \\mathbb{X}\\beta ||}^2.\n",
    "$$\n",
    "Since `numpy` does only implements unweighted least squares minimization we will therefore use the latter formulation.\n",
    "\n",
    "### Computing the AIC\n",
    "\n",
    "Up to constants independent of $S$ the log-likelihood is\n",
    "$$\n",
    "    l = \\mathbb{Y}^T\\mathbb{X}\\beta - \\sum_{i=1}^n {\\phi(\\mathbb{X}\\beta)}_i\n",
    "$$\n",
    "for $\\phi (s) = \\log(1 + e^s)$. Therefore the AIC is\n",
    "$$\n",
    "    AIC(S)\n",
    "    = l_S - |S|\n",
    "    = \\mathbb{Y}^T\\mathbb{X}_S\\beta_S - 1\\cdot\\phi(\\mathbb{X}_S\\beta_S) - |S|.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e1e241ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmoid = scipy.special.expit\n",
    "\n",
    "def augment_with_ones(array):\n",
    "    \"\"\"\n",
    "    Given an n-by-k numpy array\n",
    "    returns a n-by-(k+1) numpy array by\n",
    "    adding a first column of ones.\n",
    "    \"\"\"\n",
    "    n = array.shape[0]\n",
    "    return np.column_stack([np.ones(n), array])\n",
    "\n",
    "class LogisticRegression():\n",
    "    \"\"\"\n",
    "    dataframe: Pandas dataframe\n",
    "    response:  The header of the dataframe column used as response variable\n",
    "    model:     A list of dataframe column headers used as covariates\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, dataframe, response, model):\n",
    "        \n",
    "        self.dataframe = dataframe\n",
    "        self.response = response\n",
    "        self.model = model\n",
    "        # Response\n",
    "        self.Y = dataframe[response].to_numpy()\n",
    "        # Covariates\n",
    "        self.X = augment_with_ones(dataframe[model])\n",
    "        # Parameter estimate\n",
    "        self.beta = np.zeros(len(model) + 1)\n",
    "        # Standard error estimate of the parameter estimate\n",
    "        self.std_err = None\n",
    "        # Estimated Bernoulli parameters\n",
    "        self.p = sigmoid(np.matmul(self.X, self.beta))\n",
    "        # Matrix whose diagonal is the Fisher information of the estimated Bernoulli parameters\n",
    "        self.W = np.diag(self.p*(1-self.p))\n",
    "        # Vector used instead of Y in the weighted least squares minimization\n",
    "        self.Z = np.matmul(self.X, self.beta) + np.matmul(\n",
    "            np.linalg.inv(self.W), self.Y - self.p\n",
    "        )\n",
    "        # Inverse Hessian, or inverse Fisher information matrix\n",
    "        self.J = np.linalg.inv(\n",
    "            functools.reduce(np.matmul, [self.X.transpose(), self.W, self.X])\n",
    "        )\n",
    "        self.has_run = False\n",
    "        \n",
    "    def stopping_criterion(self):\n",
    "        \"\"\"\n",
    "        Evaluates the stopping criterion\n",
    "        (Hessian*Gradient).Gradient\n",
    "        \"\"\"\n",
    "        gradient = np.matmul(self.X.transpose(), self.Y - self.p)\n",
    "        return functools.reduce(np.matmul, [gradient.transpose(), self.J, gradient])\n",
    "    \n",
    "    def newton_descent_step(self):\n",
    "        \"\"\"\n",
    "        Performs one Newton descent step\n",
    "        characterized as a least squares minimization.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Udate several of the auxiliary tensors\n",
    "        self.p = sigmoid(np.matmul(self.X, self.beta))\n",
    "        self.W = np.diag(self.p*(1-self.p))\n",
    "        self.Z = np.matmul(self.X, self.beta) + np.matmul(\n",
    "            np.linalg.inv(self.W), self.Y - self.p\n",
    "        )\n",
    "        self.J = np.linalg.inv(\n",
    "            functools.reduce(np.matmul, [self.X.transpose(), self.W, self.X])\n",
    "        )\n",
    "        \n",
    "        # Update beta\n",
    "        self.beta, _, _, _ = np.linalg.lstsq(\n",
    "            np.matmul(np.sqrt(self.W), self.X),\n",
    "            np.matmul(np.sqrt(self.W), self.Z)\n",
    "        )\n",
    "        \n",
    "    def find_mle(self, stopping_treshold=1e-10):\n",
    "        \"\"\"\n",
    "        Perform a Newton descent until the stopping treshold is met.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.has_run = True\n",
    "        \n",
    "        # Compute beta\n",
    "        while self.stopping_criterion() > 2*stopping_treshold:\n",
    "            self.newton_descent_step()\n",
    "        \n",
    "        # Compute the standard error\n",
    "        self.std_err = np.sqrt(np.diag(self.J))\n",
    "        \n",
    "    def compute_aic(self):\n",
    "        \"\"\"\n",
    "        Return the Akaike Information Criterion.\n",
    "        \"\"\"\n",
    "        \n",
    "        def phi(s):\n",
    "            \"\"\"\n",
    "            Auxiliary function used to compute the AIC.\n",
    "            \"\"\"\n",
    "            return np.log(1 + np.exp(s))\n",
    "        \n",
    "        if not self.has_run:\n",
    "            self.find_mle()\n",
    "        \n",
    "        return (\n",
    "            functools.reduce(np.matmul, [self.Y.transpose(), self.X, self.beta])\n",
    "            - phi(np.matmul(self.X, self.beta)).sum()\n",
    "            - len(self.model)\n",
    "        )\n",
    "        \n",
    "    def report_results(self, alpha=0.05):\n",
    "        \"\"\"\n",
    "        Print a nice table with estimates and confidence intervals\n",
    "        for the parameters, as well as the AIC.\n",
    "        \"\"\"\n",
    "        \n",
    "        if not self.has_run:\n",
    "            self.find_mle()\n",
    "        \n",
    "        # Compute the Normal adjustment to the standard error estimate\n",
    "        # used to produce Normal confidence intervals\n",
    "        z = scipy.stats.norm.isf(alpha/2)\n",
    "\n",
    "        covariates_list_local = [\"Constant term\"] + self.model\n",
    "        table = [\n",
    "            [\n",
    "                covariate,\n",
    "                self.beta[j],\n",
    "                self.std_err[j],\n",
    "                self.beta[j] - z*self.std_err[j],\n",
    "                self.beta[j] + z*self.std_err[j],\n",
    "            ]\n",
    "            for j, covariate in enumerate(covariates_list_local)\n",
    "        ]\n",
    "\n",
    "        print(tabulate(\n",
    "            table,\n",
    "            headers = [\"Feature\", \"Beta_j\", \"Std. error\", \"Lower bound\", \"Upper bound\"],\n",
    "            floatfmt=\".3\" # Only print three significant digits\n",
    "        ))\n",
    "        print(f\"\\nAIC score: {self.compute_aic():.3}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57641369",
   "metadata": {},
   "source": [
    "### We first perform a logistic regression using *all* the covariates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fdbf27f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature           Beta_j    Std. error    Lower bound    Upper bound\n",
      "-------------  ---------  ------------  -------------  -------------\n",
      "Constant term  -6.15           1.31          -8.71          -3.59\n",
      "sbp             0.0065         0.00573       -0.00473        0.0177\n",
      "tobacco         0.0794         0.0266         0.0272         0.132\n",
      "ldl             0.174          0.0597         0.057          0.291\n",
      "adiposity       0.0186         0.0293        -0.0388         0.076\n",
      "famhist         0.925          0.228          0.479          1.37\n",
      "typea           0.0396         0.0123         0.0154         0.0637\n",
      "obesity        -0.0629         0.0442        -0.15           0.0238\n",
      "alcohol         0.000122       0.00448       -0.00867        0.00891\n",
      "age             0.0452         0.0121         0.0215         0.069\n",
      "\n",
      "AIC score: -2.45e+02\n"
     ]
    }
   ],
   "source": [
    "full_regression = LogisticRegression(\n",
    "    coris_df,\n",
    "    'chd',\n",
    "    ['sbp', 'tobacco', 'ldl', 'adiposity', 'famhist', 'typea', 'obesity', 'alcohol', 'age']\n",
    ")\n",
    "\n",
    "full_regression.report_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b9d978",
   "metadata": {},
   "source": [
    "### We now perform a logistic regression keeping only the covariates for which we cannot reject the null that their parameter is zero via the Wald test, which is the same thing as saying that their confidence interval contains zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a4b82fd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature          Beta_j    Std. error    Lower bound    Upper bound\n",
      "-------------  --------  ------------  -------------  -------------\n",
      "Constant term   -6.45          0.921         -8.25          -4.64\n",
      "tobacco          0.0804        0.0259         0.0297         0.131\n",
      "ldl              0.162         0.055          0.0543         0.27\n",
      "famhist          0.908         0.226          0.466          1.35\n",
      "typea            0.0371        0.0122         0.0133         0.061\n",
      "age              0.0505        0.0102         0.0305         0.0705\n",
      "\n",
      "AIC score: -2.43e+02\n"
     ]
    }
   ],
   "source": [
    "partial_regression = LogisticRegression(\n",
    "    coris_df,\n",
    "    'chd',\n",
    "    ['tobacco', 'ldl', 'famhist', 'typea', 'age']\n",
    ")\n",
    "partial_regression.report_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be933476",
   "metadata": {},
   "source": [
    "### We now turn our attention to backward stepwise regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b4a4844c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def listRemove(mylist, element):\n",
    "    return [x for x in mylist if x != element]\n",
    "\n",
    "def backward_stepwise(dataframe, response, covariates_list):\n",
    "    \n",
    "    selected_model = covariates_list\n",
    "    \n",
    "    while len(selected_model) > 0:\n",
    "        \n",
    "        model_and_score_pairs = [\n",
    "            (\n",
    "                listRemove(selected_model, covariate),\n",
    "                LogisticRegression(dataframe, response, listRemove(selected_model, covariate)).compute_aic()\n",
    "            )\n",
    "            for covariate in selected_model\n",
    "        ]\n",
    "        \n",
    "        candidate_model, candidate_model_score = max(\n",
    "            model_and_score_pairs,\n",
    "            key = lambda x: x[1] # Find the element with smallest model score\n",
    "        )\n",
    "        \n",
    "        if (candidate_model_score <= LogisticRegression(dataframe, response, selected_model).compute_aic()):\n",
    "            return selected_model\n",
    "        else:\n",
    "            selected_model = candidate_model\n",
    "    \n",
    "    return selected_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7ae9eb03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tobacco', 'ldl', 'famhist', 'typea', 'age']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_covariates_list = ['sbp', 'tobacco', 'ldl', 'adiposity', 'famhist', 'typea', 'obesity', 'alcohol', 'age']\n",
    "backward_stepwise(coris_df, 'chd', full_covariates_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d98d08de",
   "metadata": {},
   "source": [
    "### We now try *all* the possible models, then choose the model with maximal AIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a04cd19e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build all the regressions, but do not compute anything just yet\n",
    "full_covariates_list = ['sbp', 'tobacco', 'ldl', 'adiposity', 'famhist', 'typea', 'obesity', 'alcohol', 'age']\n",
    "all_regressions = [\n",
    "    LogisticRegression(coris_df, 'chd', list(model))\n",
    "    for model in more_itertools.powerset(full_covariates_list)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8d278c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform all the regressions and compute their AIC\n",
    "all_model_score_pairs = [\n",
    "    (regression.model, regression.compute_aic())\n",
    "    for regression in all_regressions\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6f24b6d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model with the largest AIC is the model with covariates\n",
      "['tobacco', 'ldl', 'famhist', 'typea', 'age'].\n",
      "Its AIC is -242.8.\n"
     ]
    }
   ],
   "source": [
    "# Report out the best model and its score\n",
    "best_model, score = max(all_model_score_pairs, key=lambda x: x[1])\n",
    "print(\n",
    "    \"The model with the largest AIC is the model with covariates\\n\"\n",
    "    f\"{best_model}.\\n\"\n",
    "    f\"Its AIC is {score:.4}.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "df015d57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-242.8  ['tobacco', 'ldl', 'famhist', 'typea', 'age']\n",
      "-243.0  ['tobacco', 'ldl', 'famhist', 'typea', 'obesity', 'age']\n",
      "-243.3  ['sbp', 'tobacco', 'ldl', 'famhist', 'typea', 'obesity', 'age']\n",
      "-243.3  ['sbp', 'tobacco', 'ldl', 'famhist', 'typea', 'age']\n"
     ]
    }
   ],
   "source": [
    "# If, for whatever reason, we seek the top N models with highest score\n",
    "N = 4\n",
    "top_models = sorted(\n",
    "    all_model_score_pairs,\n",
    "    key=lambda x: -x[1]\n",
    ")[:N]\n",
    "for model, score in top_models:\n",
    "    print(f\"{score:.4}  {model}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
