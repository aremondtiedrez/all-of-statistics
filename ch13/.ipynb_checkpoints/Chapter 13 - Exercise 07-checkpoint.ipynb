{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3eaae63f",
   "metadata": {},
   "source": [
    "Get the passenger car mileage data from\n",
    "https://lib.stat.cmu.edu/DASL/Datafiles/carmpgdat.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a9049b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "from tabulate import tabulate\n",
    "\n",
    "import more_itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ce24fac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MAKE/MODEL</th>\n",
       "      <th>VOL</th>\n",
       "      <th>HP</th>\n",
       "      <th>MPG</th>\n",
       "      <th>SP</th>\n",
       "      <th>WT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GM/GeoMetroXF1</td>\n",
       "      <td>89</td>\n",
       "      <td>49</td>\n",
       "      <td>65.4</td>\n",
       "      <td>96</td>\n",
       "      <td>17.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GM/GeoMetro</td>\n",
       "      <td>92</td>\n",
       "      <td>55</td>\n",
       "      <td>56.0</td>\n",
       "      <td>97</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GM/GeoMetroLSI</td>\n",
       "      <td>92</td>\n",
       "      <td>55</td>\n",
       "      <td>55.9</td>\n",
       "      <td>97</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SuzukiSwift</td>\n",
       "      <td>92</td>\n",
       "      <td>70</td>\n",
       "      <td>49.0</td>\n",
       "      <td>105</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DaihatsuCharade</td>\n",
       "      <td>92</td>\n",
       "      <td>53</td>\n",
       "      <td>46.5</td>\n",
       "      <td>96</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>Mercedes500SL</td>\n",
       "      <td>50</td>\n",
       "      <td>322</td>\n",
       "      <td>18.1</td>\n",
       "      <td>165</td>\n",
       "      <td>45.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>Mercedes560SEL</td>\n",
       "      <td>115</td>\n",
       "      <td>238</td>\n",
       "      <td>17.2</td>\n",
       "      <td>140</td>\n",
       "      <td>45.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>JaguarXJSConvert</td>\n",
       "      <td>50</td>\n",
       "      <td>263</td>\n",
       "      <td>17.0</td>\n",
       "      <td>147</td>\n",
       "      <td>45.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>BMW750IL</td>\n",
       "      <td>119</td>\n",
       "      <td>295</td>\n",
       "      <td>16.7</td>\n",
       "      <td>157</td>\n",
       "      <td>45.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>Rolls-RoyceVarious</td>\n",
       "      <td>107</td>\n",
       "      <td>236</td>\n",
       "      <td>13.2</td>\n",
       "      <td>130</td>\n",
       "      <td>55.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>82 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            MAKE/MODEL  VOL   HP   MPG   SP    WT\n",
       "0       GM/GeoMetroXF1   89   49  65.4   96  17.5\n",
       "1          GM/GeoMetro   92   55  56.0   97  20.0\n",
       "2       GM/GeoMetroLSI   92   55  55.9   97  20.0\n",
       "3          SuzukiSwift   92   70  49.0  105  20.0\n",
       "4      DaihatsuCharade   92   53  46.5   96  20.0\n",
       "..                 ...  ...  ...   ...  ...   ...\n",
       "77       Mercedes500SL   50  322  18.1  165  45.0\n",
       "78      Mercedes560SEL  115  238  17.2  140  45.0\n",
       "79    JaguarXJSConvert   50  263  17.0  147  45.0\n",
       "80            BMW750IL  119  295  16.7  157  45.0\n",
       "81  Rolls-RoyceVarious  107  236  13.2  130  55.0\n",
       "\n",
       "[82 rows x 6 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the data into a pandas data frame\n",
    "car_mileage_df = pd.read_csv('../data/passenger_car_mileage_clean.dat', sep='\\t')\n",
    "\n",
    "# Print the data frame, as a sanity check\n",
    "car_mileage_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "633a0c66",
   "metadata": {},
   "source": [
    "### (a) Fit a multiple linear regression model to predict MPG (miles per gallon) from the other variables. Summarize your analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb3ee4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Linear_regression_result = namedtuple('Linear_regression_result', [\n",
    "    'X', 'Y', 'beta', 'rss', 'variance', 'beta_std_err', 'wald_statistic'\n",
    "])\n",
    "\n",
    "def linear_regression(X, Y):\n",
    "    \n",
    "    if len(X) > 0: # Check whether or not X is empty\n",
    "        # Record the number of samples and the dimension of the model    \n",
    "        (n, k) = X.shape\n",
    "        \n",
    "        # Add a column of ones to X (to account for the affine term in the linear regression)\n",
    "        X_affine = np.column_stack([np.ones(n), X])    \n",
    "    \n",
    "        # Compute the least squares estimate and\n",
    "        # the residual sum of squares\n",
    "        beta, rss, _, _ = np.linalg.lstsq(X_affine, Y)\n",
    "\n",
    "        # Estimate the variance of the noise term\n",
    "        variance = rss[0]/(n-k-1)\n",
    "\n",
    "        # Estimate the standard errors for each parameter\n",
    "        beta_std_err = variance*np.diagonal(np.linalg.inv(np.matmul(X_affine.transpose(), X_affine)))\n",
    "        \n",
    "    else:   \n",
    "        \n",
    "        # When X is empty we are performing a \"zero-dimensional\"\n",
    "        # linear regression, i.e. approximating Y with its mean.\n",
    "     \n",
    "        (n, ) = Y.shape\n",
    "        beta = np.array(Y.mean())\n",
    "        rss = [np.sum(np.square(Y- Y.mean()))]\n",
    "        variance = rss[0]/(n-1)\n",
    "        beta_std_err = np.array(variance/n)\n",
    "\n",
    "    # Compute the Wald test statistic for beta_j \\neq 0\n",
    "    # (used in the Zheng-Loh model selection method)\n",
    "    wald_statistic = beta/beta_std_err\n",
    "\n",
    "    return Linear_regression_result(\n",
    "        X, Y, beta, rss, variance, beta_std_err, wald_statistic\n",
    "    )\n",
    "    \n",
    "def report_result(result: Linear_regression_result, covariates_list, dash_length, alpha=0.05):\n",
    "    \n",
    "    # Compute the Normal adjustment to the standard error estimate\n",
    "    # used to produce Normal confidence intervals\n",
    "    z = scipy.stats.norm.isf(alpha/2)\n",
    "\n",
    "    # Report out the result\n",
    "    print(dash_length*\"-\")\n",
    "    print(f\"The estimate of the noise variance is {result.variance:.3}.\")\n",
    "\n",
    "    covariates_list_local = [\"Constant term\"] + covariates_list\n",
    "    table = [\n",
    "        [\n",
    "            covariate,\n",
    "            result.beta[j],\n",
    "            result.beta_std_err[j],\n",
    "            result.beta[j] - z*result.beta_std_err[j],\n",
    "            result.beta[j] + z*result.beta_std_err[j],\n",
    "        ]\n",
    "        for j, covariate in enumerate(covariates_list_local)\n",
    "    ]\n",
    "\n",
    "    print(dash_length*\"-\")\n",
    "    print(tabulate(\n",
    "        table,\n",
    "        headers = [\"Feature\", \"Beta_j\", \"Std. error\", \"Lower bound\", \"Upper bound\"],\n",
    "        floatfmt=\".3\" # Only print three significant digits\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e7753fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------\n",
      "The estimate of the noise variance is 13.3.\n",
      "--------------------------------------------------------------------\n",
      "Feature           Beta_j    Std. error    Lower bound    Upper bound\n",
      "-------------  ---------  ------------  -------------  -------------\n",
      "Constant term   1.92e+02      5.54e+02      -8.93e+02       1.28e+03\n",
      "VOL            -0.0156        0.000521      -0.0167        -0.0146\n",
      "HP              0.392         0.00663        0.379          0.405\n",
      "SP             -1.29          0.0599        -1.41          -1.18\n",
      "WT             -1.86          0.0455        -1.95          -1.77\n"
     ]
    }
   ],
   "source": [
    "covariates_list = ['VOL', 'HP', 'SP', 'WT']\n",
    "result = linear_regression(\n",
    "    X=car_mileage_df[covariates_list].to_numpy(),\n",
    "    Y=car_mileage_df['MPG'].to_numpy()\n",
    ")\n",
    "report_result(result, covariates_list, dash_length=68)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efaaef48",
   "metadata": {},
   "source": [
    "### Summary of results\n",
    "1. The confidence interval for all the parameters (except beta_0)\n",
    "   are sufficiently far from zero for us to argue that there is\n",
    "   significant statistical evidence that these parameters are nonzero.\n",
    "2. We cannot, however, reject the null hypothesis that the constant term\n",
    "   is zero.\n",
    "3. A **major** warning with this analysis is the following:\n",
    "   the different features are **not** expected to be independent!\n",
    "   (How to argue this more precisely will be seen in the next chapter.)\n",
    "   \n",
    "   This does not cause any issues with *using* the model\n",
    "   (since there is no need to assume that a single draw $X$ has independent components),\n",
    "   however it *does* mean that we have to be very careful when *interpreting*\n",
    "   the values $\\beta_j$.\n",
    "   \n",
    "   For example: looking at $\\beta_2 = \\beta_{HP}$ we may be naively tempted\n",
    "   to say that, as the horsepower (HP) increases, the miles per gallon (MPG) also increase.\n",
    "   However, as HP increases, we would also expect other values\n",
    "   such as SP (top speed) and WT (weight) to increase, which when all considered\n",
    "   together *may* lead to a *decrease* in MPG!\n",
    "   (Which would be consistent with the *simple* regression analysis carried out\n",
    "   in Exercise 06 which showed that, if we *only* considered HP as a covariate,\n",
    "   then MPG decreased as a function of HP!)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b994216",
   "metadata": {},
   "source": [
    "### (b) Use Mallows' $C_p$ to select a best sub-model. To search through the models try (i) forward stepwise, (ii) backward stepwise. Summarize your findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67100744",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mallows' Cp statistic\n",
    "def mallows(result, model, variance):\n",
    "    \"\"\"\n",
    "    model is a sublist (possibly empty)\n",
    "    of covariates_list\n",
    "    \"\"\"\n",
    "    return result.rss[0] + 2*len(model)*variance\n",
    "\n",
    "def retrieve_covariate_data(dataframe, model):\n",
    "    \"\"\"\n",
    "    This functions does one thing and one thing only:\n",
    "    it makes it so that, whether or not the model is empty,\n",
    "    we obtain the covariates X via\n",
    "    X = retrieve_covariate_data(dataframe, model)\n",
    "    \"\"\"\n",
    "    if len(model) == 0:\n",
    "        return np.array([])\n",
    "    else:\n",
    "        return dataframe[model].to_numpy()\n",
    "    \n",
    "def model_score(dataframe, response, model, score_function):\n",
    "    \n",
    "    full_variance = linear_regression(\n",
    "        X=dataframe[covariates_list].to_numpy(),\n",
    "        Y=dataframe[response].to_numpy()\n",
    "    ).variance\n",
    "    \n",
    "    score = score_function(\n",
    "        linear_regression(\n",
    "            X=retrieve_covariate_data(dataframe, model),\n",
    "            Y=dataframe[response].to_numpy(),\n",
    "        ),\n",
    "        model,\n",
    "        full_variance\n",
    "    )\n",
    "    \n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef23e65b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_stepwise(dataframe, response, covariates_list):\n",
    "    \n",
    "    k = len(covariates_list)\n",
    "    selected_model = []\n",
    "    \n",
    "    while len(selected_model) < k:\n",
    "        \n",
    "        model_and_score_pairs = [\n",
    "            (\n",
    "                selected_model + [covariate],\n",
    "                model_score(dataframe, response, selected_model + [covariate], mallows)\n",
    "            )\n",
    "            for covariate in covariates_list\n",
    "            if covariate not in selected_model\n",
    "        ]\n",
    "        \n",
    "        candidate_model, candidate_model_score = min(\n",
    "            model_and_score_pairs,\n",
    "            key = lambda x: x[1] # Find the element with smallest model score\n",
    "        )\n",
    "        \n",
    "        if candidate_model_score >= model_score(dataframe, response, selected_model, mallows):\n",
    "            return selected_model\n",
    "        else:\n",
    "            selected_model = candidate_model\n",
    "    \n",
    "    return selected_model\n",
    "\n",
    "def listRemove(mylist, element):\n",
    "    return [x for x in mylist if x != element]\n",
    "\n",
    "def backward_stepwise(dataframe, response, covariates_list):\n",
    "    \n",
    "    selected_model = covariates_list\n",
    "    \n",
    "    while len(selected_model) > 0:\n",
    "        \n",
    "        model_and_score_pairs = [\n",
    "            (\n",
    "                listRemove(selected_model, covariate),\n",
    "                model_score(dataframe, response, listRemove(selected_model, covariate), mallows)\n",
    "            )\n",
    "            for covariate in selected_model\n",
    "        ]\n",
    "        \n",
    "        candidate_model, candidate_model_score = min(\n",
    "            model_and_score_pairs,\n",
    "            key = lambda x: x[1] # Find the element with smallest model score\n",
    "        )\n",
    "        \n",
    "        if candidate_model_score >= model_score(dataframe, response, selected_model, mallows):\n",
    "            return selected_model\n",
    "        else:\n",
    "            selected_model = candidate_model\n",
    "    \n",
    "    return selected_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "57f4b019",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are the models selected by forward and\n",
      "backward stepwise regression:\n",
      "\n",
      "Forward stepwise: ['WT', 'SP', 'HP'].\n",
      "Backward stepwise: ['HP', 'SP', 'WT'].\n",
      "\n",
      "Note that both methods yield the same model.\n",
      "\n",
      "\n",
      "We now use the model obtained by forward/backward stepwise regression.\n",
      "----------------------------------------------------------------------\n",
      "The estimate of the noise variance is 13.3.\n",
      "----------------------------------------------------------------------\n",
      "Feature           Beta_j    Std. error    Lower bound    Upper bound\n",
      "-------------  ---------  ------------  -------------  -------------\n",
      "Constant term   1.94e+02      5.44e+02      -8.72e+02       1.26e+03\n",
      "WT             -1.92          0.037         -1.99          -1.85\n",
      "SP             -1.32          0.0582        -1.43          -1.21\n",
      "HP              0.405         0.00623        0.393          0.417\n",
      "\n",
      "\n",
      "Finally we compare the scores of the full model and\n",
      "the selected model using Mallows' Cp statistic.\n",
      "\n",
      "Score of full model :     1.13e+03.\n",
      "Score of selected model : 1.11e+03.\n",
      "Score improvement:        1.8%.\n"
     ]
    }
   ],
   "source": [
    "# Full list of covariates\n",
    "covariates_list = ['VOL', 'HP', 'SP', 'WT']\n",
    "\n",
    "print(\n",
    "    \"Here are the models selected by forward and\\n\"\n",
    "    \"backward stepwise regression:\\n\\n\"\n",
    "    f\"Forward stepwise: {forward_stepwise(car_mileage_df, 'MPG', covariates_list)}.\\n\"\n",
    "    f\"Backward stepwise: {backward_stepwise(car_mileage_df, 'MPG', covariates_list)}.\\n\\n\"\n",
    "    \"Note that both methods yield the same model.\\n\\n\"\n",
    ")\n",
    "\n",
    "print(\"We now use the model obtained by forward/backward stepwise regression.\")\n",
    "selected_model = forward_stepwise(car_mileage_df, 'MPG', covariates_list)\n",
    "result = linear_regression(\n",
    "    X=car_mileage_df[selected_model].to_numpy(),\n",
    "    Y=car_mileage_df['MPG'].to_numpy()\n",
    ")\n",
    "report_result(result, selected_model, dash_length=70)\n",
    "\n",
    "\n",
    "full_score = model_score(car_mileage_df, 'MPG', covariates_list, mallows)\n",
    "selected_score = model_score(car_mileage_df, 'MPG', selected_model, mallows)\n",
    "print(\n",
    "    \"\\n\\n\"\n",
    "    \"Finally we compare the scores of the full model and\\n\"\n",
    "    \"the selected model using Mallows' Cp statistic.\\n\\n\"\n",
    "    f\"Score of full model :     {full_score:.3}.\\n\"\n",
    "    f\"Score of selected model : {selected_score:.3}.\\n\"\n",
    "    f\"Score improvement:        {(full_score-selected_score)/full_score*100:.3}%.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5dd306a",
   "metadata": {},
   "source": [
    "### Summary of results\n",
    "1. Both forward and backward stepwise regression select the same model.\n",
    "2. The only covariate dropped from the full model is `VOL: Cubic feet of cab space`.\n",
    "3. The improvement in Mallows' $C_p$ statistic, between using the full model\n",
    "   or the model selected by forward/backward stepwise regression, is fairly marginal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c257592",
   "metadata": {},
   "source": [
    "### (c) Use the Zheng-Loh model selection method and compare to (b)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "15c4dec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ordered_sublist(ordered_list):\n",
    "    \n",
    "    return [\n",
    "        ordered_list[:i]\n",
    "        for i in range(0, len(ordered_list)+1)\n",
    "    ]\n",
    "\n",
    "def zl_model_selection(dataframe, response, covariates_list):\n",
    "    \n",
    "    # Perform the full linear regression\n",
    "    result = linear_regression(\n",
    "        X=dataframe[covariates_list].to_numpy(),\n",
    "        Y=dataframe[response].to_numpy()\n",
    "    )\n",
    "    \n",
    "    # Pair the covariates with their Wald test statistics\n",
    "    covariate_wald_statistic_pairs = list(zip(\n",
    "        covariates_list, \n",
    "        np.abs(result.wald_statistic[1:])\n",
    "    ))\n",
    "    \n",
    "    # Sort by the Wald test statistics,\n",
    "    # i.e. by the tuple elements with indices i=1\n",
    "    ordered_covariate_wald_statistics_pairs = sorted(\n",
    "        covariate_wald_statistic_pairs,\n",
    "        key=lambda x: -x[1]\n",
    "    )\n",
    "    # Keep only the covariates\n",
    "    # (forgetting the Wald test statistics)\n",
    "    ordered_covariates = [\n",
    "        covariate \n",
    "        for covariate, wald_statistic\n",
    "        in ordered_covariate_wald_statistics_pairs\n",
    "    ]\n",
    "    \n",
    "    # List the models [], ['Cov1'], ['Cov1', 'Cov2']\n",
    "    # in order of increasing size, where Cov1, Cov2, ...\n",
    "    # are the covariates in order of decreasin absolute Wald test statistic.\n",
    "    ordered_model_list = ordered_sublist(ordered_covariates)\n",
    "    \n",
    "    # Compute the Zheng-Log model score for each sub-model\n",
    "    n = len(dataframe[response].to_numpy())\n",
    "    full_variance = result.variance\n",
    "    \n",
    "    zl_optimal_model_size = np.argmin([\n",
    "        model_score(dataframe, response, model, mallows)\n",
    "        + a*full_variance*np.log(n)\n",
    "        for a, model in enumerate(ordered_model_list)\n",
    "    ])\n",
    "    \n",
    "    return ordered_model_list[zl_optimal_model_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "498af2e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['HP', 'WT', 'VOL', 'SP']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zl_model_selection(car_mileage_df, 'MPG', ['VOL', 'HP', 'SP', 'WT'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01630b3b",
   "metadata": {},
   "source": [
    "### Summary of results\n",
    "The Zheng-Loh model selection chooses the *full* model,\n",
    "i.e. not dropping a single covariate.\n",
    "\n",
    "In particular the Zheng-Loh model chooses a *different*\n",
    "model than forward/backward stepwise regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b2e39c",
   "metadata": {},
   "source": [
    "### (d) Perform all possible regression. Compare $C_p$ and BIC. Compare the results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68d5579",
   "metadata": {},
   "source": [
    "In order to use the Bayesian information criterion,\n",
    "since we do **not** know the form of the likelihood function\n",
    "we must make an **assumption**.\n",
    "Namely we assume the *Normal noise assumption*,\n",
    "which says that the noise term $\\varepsilon = Y - \\beta\\cdot X$\n",
    "has a Normal distribution when conditioned on $X$, with constant variance.\n",
    "\n",
    "In that context, we know that Mallow's $C_p$ statistic is, up to a constant,\n",
    "equal to the Akaike information Criterion, which thus lets us write\n",
    "the AIC, and hence the BIC, in terms of the training error.\n",
    "\n",
    "More precisely: (where we cheat here and use the *estimate* $\\hat\\sigma$ even though we only\n",
    "know that Mallows' $C_p$ statistic and the AIC agree when $\\sigma$ is *known*)\n",
    "$$\n",
    "    l_S - |S|\n",
    "    = AIC(S)\n",
    "    = -\\frac{1}{2\\hat\\sigma^2} \\hat{R} (S) + C\n",
    "    = -\\frac{1}{2\\hat\\sigma^2} \\hat{R}_{tr} (S) - |S| + C\n",
    "$$\n",
    "for some constant $C$ independent of the model $S$,\n",
    "and so\n",
    "$$\n",
    "    l_S = -\\frac{1}{2\\hat\\sigma^2} \\hat{R}_{tr} (S) + C\n",
    "$$\n",
    "So finally, up to an irrelevant constant,\n",
    "$$\n",
    "    BIC \n",
    "    = l_S - \\frac{|S|}{2} \\log n\n",
    "    = -\\frac{1}{2\\hat\\sigma^2} \\hat{R}_{tr} (S) - \\frac{|S|}{2} \\log n.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "62eca297",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NEGATIVE Bayesian information criterion\n",
    "# (we take the negative BIC, with opposite sign,\n",
    "# to be able to still select the best model\n",
    "# by minimizing the score)\n",
    "def bic(result, model, variance):\n",
    "    \"\"\"\n",
    "    model is a sublist (possibly empty)\n",
    "    of covariates_list\n",
    "    \"\"\"\n",
    "    (n, ) = result.Y.shape\n",
    "    return result.rss[0]/(2*variance) + np.log(n)*len(model)/2\n",
    "\n",
    "def optimal_model_selection(dataframe, response, covariates_list, score_function):\n",
    "    \n",
    "    # Perform the full linear regression\n",
    "    full_variance = linear_regression(\n",
    "        X=dataframe[covariates_list].to_numpy(),\n",
    "        Y=dataframe[response].to_numpy()\n",
    "    ).variance\n",
    "    \n",
    "    # Pair together all the possible submodels with their score,\n",
    "    # then find the model with minimal score.\n",
    "    return min(\n",
    "        [\n",
    "            (model, model_score(dataframe, response, list(model), score_function))\n",
    "            for model in more_itertools.powerset(covariates_list)\n",
    "        ],\n",
    "        key=lambda x: x[1]\n",
    "    )[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6ba7bfc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal model using different score functions:\n",
      "Mallows' Cp statistics:         ('HP', 'SP', 'WT')\n",
      "Bayesian information criterion: ('HP', 'SP', 'WT')\n"
     ]
    }
   ],
   "source": [
    "optimal_model = {\n",
    "    \"mallows\": optimal_model_selection(car_mileage_df, 'MPG', ['VOL', 'HP', 'SP', 'WT'], mallows),\n",
    "    \"bic\": optimal_model_selection(car_mileage_df, 'MPG', ['VOL', 'HP', 'SP', 'WT'], bic)\n",
    "}\n",
    "\n",
    "print(\n",
    "    \"Optimal model using different score functions:\\n\"\n",
    "    f\"Mallows' Cp statistics:         {optimal_model[\"mallows\"]}\\n\"\n",
    "    f\"Bayesian information criterion: {optimal_model[\"bic\"]}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a3035108",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cp: 8107     BIC: 304     Model: ().\n",
      "Cp: 7033     BIC: 265     Model: ('VOL',).\n",
      "Cp: 3076     BIC: 116     Model: ('HP',).\n",
      "Cp: 4292     BIC: 162     Model: ('SP',).\n",
      "Cp: 1493     BIC:  57     Model: ('WT',).\n",
      "Cp: 2328     BIC:  90     Model: ('VOL', 'HP').\n",
      "Cp: 3030     BIC: 116     Model: ('VOL', 'SP').\n",
      "Cp: 1515     BIC:  59     Model: ('VOL', 'WT').\n",
      "Cp: 2410     BIC:  93     Model: ('HP', 'SP').\n",
      "Cp: 1484     BIC:  58     Model: ('HP', 'WT').\n",
      "Cp: 1436     BIC:  56     Model: ('SP', 'WT').\n",
      "Cp: 2121     BIC:  83     Model: ('VOL', 'HP', 'SP').\n",
      "Cp: 1481     BIC:  59     Model: ('VOL', 'HP', 'WT').\n",
      "Cp: 1417     BIC:  57     Model: ('VOL', 'SP', 'WT').\n",
      "Cp: 1114     BIC:  45     Model: ('HP', 'SP', 'WT').\n",
      "Cp: 1134     BIC:  47     Model: ('VOL', 'HP', 'SP', 'WT').\n"
     ]
    }
   ],
   "source": [
    "# Print ALL possible models, along with their scores\n",
    "for model in more_itertools.powerset(covariates_list):\n",
    "    print(\n",
    "        f\"Cp: {model_score(car_mileage_df, 'MPG', list(model), mallows):.0f}\" + 5*\" \"\n",
    "        + f\"BIC: {model_score(car_mileage_df, 'MPG', list(model), bic):3.0f}\" + 5*\" \"\n",
    "        + f\"Model: {model}.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda19089",
   "metadata": {},
   "source": [
    "### Summary of results\n",
    "Using Mallows' $C_p$ statistic or the Bayesian information criterion\n",
    "selects the same model (which was also selected by forward/backward stepwise regression).\n",
    "\n",
    "Nonetheless, we *do* see some slight preference from the BIC\n",
    "for sparser models (i.e. models with fewer covariates).\n",
    "Indeed: according to both scores the optimal model, then the full model,\n",
    "are the top two models but, after that, slight differences appear.\n",
    "\n",
    " - The BIC prefers, in third position, the model `SP + WT`.\n",
    " - Mallows' $C_p$ statistic prefers, in third position, the model `VOL + SP + WT`.\n",
    " \n",
    "Note however that these same models are then in fourth position for the other scoring method,\n",
    "so the differences are indeed slight."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
